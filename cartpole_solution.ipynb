{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Playing CartPole with Actor-Critic Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 15:56:32.833628: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import collections\n",
    "import tqdm\n",
    "import statistics\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arch/miniconda3/envs/python_ai/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# creating environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# for experiment reproducibility\n",
    "env.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"Combined actor-critic network\"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, num_hidden_units: int):\n",
    "        \"\"\"initialize\"\"\"\n",
    "        super().__init__()\n",
    "        self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)        # agent can take 2 actions to push cart: left(0) & right(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_hidden_units we have taken 128 \n",
    "model = ActorCritic(env.action_space.n, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COLLECTING TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  return (state.astype(np.float32), np.array(reward, np.int32), np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "  return tf.numpy_function(env_step, [action], [tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "        initial_state: tf.Tensor,\n",
    "        model: tf.keras.Model,\n",
    "        max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "\n",
    "  for t in tf.range(max_steps):\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    action_logits_t, value = model(state)\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "    state, reward, done = tf_env_step(action)\n",
    "    state.set_shape(initial_state_shape)\n",
    "    rewards = rewards.write(t, reward)\n",
    "    if tf.cast(done, tf.bool):\n",
    "      break\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "\n",
    "  return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTING EXPECTED RETURNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "        rewards: tf.Tensor,\n",
    "        gamma: float,\n",
    "        standardize: bool = True) -> tf.Tensor:\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) /\n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTOR-CRITIC LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "\n",
    "def compute_loss(\n",
    "        action_probs: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        returns: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINING TRAINING STEP TO UPDATE PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "        initial_state: tf.Tensor,\n",
    "        model: tf.keras.Model,\n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        gamma: float,\n",
    "        max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards = run_episode(\n",
    "        initial_state, model, max_steps_per_episode)\n",
    "\n",
    "    # Calculate expected returns\n",
    "    returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, returns = [\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
    "\n",
    "    # Calculating loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "  # Compute the gradients from the loss\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "  return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN THE TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]2022-07-17 15:57:12.197894: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-07-17 15:57:12.231333: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2601325000 Hz\n",
      "2022-07-17 15:57:12.469657: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-07-17 15:57:13.493742: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "Episode 361:   4%|â–Ž         | 361/10000 [00:54<24:17,  6.61it/s, episode_reward=200, running_reward=195] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 361: average reward: 195.38!\n",
      "CPU times: user 2min 22s, sys: 17.4 s, total: 2min 40s\n",
      "Wall time: 54.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100\n",
    "# consecutive trials\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(\n",
    "    maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "  for i in t:\n",
    "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "    episode_reward = int(train_step(\n",
    "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "    t.set_description(f'Episode {i}')\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "    # Show average episode reward every 10 episodes\n",
    "    if i % 10 == 0:\n",
    "      pass  # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
    "        break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render an episode and save as a GIF file\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "\n",
    "Display(visible=0, size=(400, 300)).start()\n",
    "\n",
    "\n",
    "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int):\n",
    "  screen = env.render(mode='rgb_array')\n",
    "  im = Image.fromarray(screen)\n",
    "\n",
    "  images = [im]\n",
    "\n",
    "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "  for i in range(1, max_steps + 1):\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    action_probs, _ = model(state)\n",
    "    action = np.argmax(np.squeeze(action_probs))\n",
    "\n",
    "    state, _, done, _ = env.step(action)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "\n",
    "    # Render screen every 2 steps\n",
    "    if i % 2 == 0:\n",
    "      screen = env.render(mode='rgb_array')\n",
    "      images.append(Image.fromarray(screen))\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  return images\n",
    "\n",
    "\n",
    "# Save GIF image\n",
    "images = render_episode(env, model, max_steps_per_episode)\n",
    "image_file = 'cartpole-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('python_ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5af77f05d3bdf18f34d2227fcd1c3aa7be868e46f5c9d85f13e352356a74cc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
